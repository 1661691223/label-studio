---
title: Prompts overview
short: Prompts overview
tier: enterprise
type: guide
order: 0
order_enterprise: 225
meta_title: Prompts overview
meta_description: An overview of the Prompts feature in the HumanSignal platform
section: Prompts
date: 2024-05-15 14:30:14
---


Use Prompts to evaluate and refine your LLM prompts, and then generate predictions to automate your labeling process. 

All you need to get started is an OpenAI API key and a dataset with some ground truth annotations. 

With Prompts, you can:

* Drastically increase the speed and efficiency of annotations, transforming subject matter experts (SMEs) into highly productive data scientists while reducing the dependency on non-SME manual annotators.
* Enhance annotation throughput and accuracy, making the process faster and more scalable. 
* Empower users to harness the full potential of AI-driven text labeling, setting a new standard for efficiency and innovation in data labeling.

## Auto-labeling with Prompts
 
Prompts allows you to leverage GenAI to swiftly generate accurate predictions, enabling instant labeling of thousands of tasks. 

By leveraging AI to handle the bulk of the annotation work, you can significantly enhance the efficiency and speed of your data labeling workflows. This is particularly valuable when dealing with large datasets that require consistent and accurate labeling. Automating this process reduces the reliance on manual annotators, which not only cuts down on labor costs but also minimizes human errors and biases. With AI's ability to learn from the provided ground truth annotations, you can maintain a high level of accuracy and consistency across the dataset, ensuring high-quality labeled data for training machine learning models.



## Prompt evaluation and fine-tuning

!!! info Tip
    While you can use this tool to do basic prompt evaluation and fine-tuning, we recommend using [Evals](evals_overview), which has been specifically designed for this purpose and has much more robust analytics. 

As you evaluate your prompt against the ground truth annotations, we return an accuracy score for each version of your prompt. You can use this to iterate your prompt versions for [clarity, specificity, and context](prompts_draft#Drafting-effective-prompts). 

![Screenshot of accuracy score](/images/prompts/accuracy_score.png)

This accuracy score provides a measurable way to evaluate and refine the performance of your prompt. By tracking accuracy, you can ensure that the automated labels generated by the LLM are consistent with ground truth data. 

This feedback loop allows you to iteratively fine-tune your prompts, optimizing the accuracy of predictions and enhancing the overall reliability of your data annotation processes. In industries where data accuracy directly impacts decision-making and operational efficiency, this capability is invaluable.

## Model types

### Text classification 

At present, the Prompts feature supports text classification labeling tasks.  

Text classification is the process of assigning predefined categories or labels to segments of text based on their content. This involves analyzing the text and determining which category or label best describes its subject, sentiment, or purpose. The goal is to organize and categorize textual data in a way that makes it easier to analyze, search, and utilize. 

Text classification labeling tasks are fundamental in many applications, enabling efficient data organization, improving searchability, and providing valuable insights through data analysis. Some examples include:

* **Spam Detection**: Classifying emails as "spam" or "not spam."
* **Sentiment Analysis**: Categorizing user reviews as "positive," "negative," or "neutral."
* **Topic Categorization**: Assigning articles to categories like "politics," "sports," "technology," etc.
* **Support Ticket Classification**: Labeling customer support tickets based on the issue type, such as "billing," "technical support," or "account management."
* **Content Moderation**: Identifying and labeling inappropriate content on social media platforms, such as "offensive language," "hate speech," or "harassment."


## Workflow

1. Create a project and import a text-based dataset. 

    * [Create a project](setup_project)
    * [Sync data from external storage](storage)
2. Annotate a subset of tasks, marking as many as possible as ground truth annotation. The more ground truths you have in your dataset, the better your results will be. 

    * [Labeling guide](labeling)
    * [Define ground truth annotations for a project](quality#Define-ground-truth-annotations-for-a-project)
    * [Blog - What's a ground truth dataset?](https://humansignal.com/blog/what-s-a-ground-truth-dataset/)
3. Go to the Prompts page and create a new model. The first time you create a model, you will be asked to provide an OpenAI API key. 

    * [Create a model](prompts_model)
    * [Where do I find my OpenAI API Key?](https://help.openai.com/en/articles/4936850-where-do-i-find-my-openai-api-key)
4. Write a prompt and evaluate it against your ground truth dataset. 

    * [Draft a prompt](prompt_draft)
5. When your prompt is returning an overall accuracy that is acceptable, you can apply it to the rest of the tasks in your project. 

    * [Generate predictions from a prompt](prompts_predictions)

![Diagram of auto-labeling workflow](/images/prompts/prompter-diagram.png)

## Features, requirements, and constraints

<div class="noheader rowheader">

| Feature | Support |
| --- | --- |
| **Supported data types** | Text |
| **Supported model types** | Text Classification |
| **Supported base models** | OpenAI gpt-3.5-turbo-16k <br>OpenAI gpt-3.5-turbo-instruct <br>OpenAI gpt-4-turbo <br>OpenAI gpt-3.5-turbo <br>OpenAI gpt-4o <br>OpenAI gpt-4 |
| **Number of API keys per org** | 1 (OpenAI API key only) |
| **Required permissions** | **Owners, Administrators, Managers** -- Can create Prompt models and update projects with auto-annotations. Managers can only apply models to projects in which they are already a member. <br><br>**Reviewers and Annotators** -- No access to the Prompts tool, but can see the predictions generated by the prompts from within the project (depending on your [project settings](project_settings_lse)).  |
| **Enterprise vs. Open Source** | Label Studio Enterprise only |

</div>




